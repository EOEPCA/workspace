{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Workspace Building Block (BB) provides a unified environment where large amounts of data may become instantly accessible, analysable, and shareable. It combines object storage, interactive runtimes, and collaborative tooling into a single Kubernetes-native platform \u2014 built on Crossplane v2 and fully integrated with Keycloak for identity and access control.</p> <p>Workspaces enable individuals, teams, and organisations to provision isolated, self-service environments for data access, algorithm development, and collaborative exploration \u2014 all declaratively managed on Kubernetes and orchestrated through the Workspace REST API or an intuitive web interface built on top of it.</p>"},{"location":"#key-capabilities","title":"Key Capabilities","text":""},{"location":"#unified-storage-and-runtime","title":"Unified Storage and Runtime","text":"<p>Each workspace integrates persistent object storage (via provider-storage) and interactive compute environments (via provider-datalab).</p> <p>Users can browse data, launch code editors or terminals, and generate secure share links directly from their Datalab \u2014 without leaving the browser.</p>"},{"location":"#declarative-lifecycle-management","title":"Declarative Lifecycle Management","text":"<p>Workspaces are managed through Crossplane compositions, ensuring reproducible provisioning and continuous reconciliation. Storage, runtime, and IAM components are described as manifests and can either be orchestrated by the Workspace API or be deployed manually, via API, or through GitOps tools such as Flux or ArgoCD.</p> <pre><code>kubectl get storage -A\nNAMESPACE   NAME        SYNCED   READY   COMPOSITION     AGE\nworkspace   ws-alice    True     True    storage-minio   8d\nworkspace   ws-bob      True     True    storage-minio   8d\nworkspace   ws-eric     True     True    storage-minio   8d\n</code></pre>"},{"location":"#secure-collaboration","title":"Secure Collaboration","text":"<p>Built-in Keycloak integration ensures unified authentication and fine-grained access control. Workspace owners can invite collaborators and manage shared storage by granting or revoking access permissions as needed. Upcoming releases will introduce vended credentials for scoped, time-limited access tokens.</p>"},{"location":"#extensible-by-design","title":"Extensible by Design","text":"<p>Datalab environments can be curated and customised by teams themselves, allowing them to adapt the workspace to their individual needs. Running on top of Kubernetes, each Datalab provides the flexibility to deploy additional services \u2014 such as catalogues, dashboards, or experiment-tracking tools \u2014 directly through the Kubernetes API (e.g. using <code>kubectl</code>).  </p> <p>Operators can decide whether to expose a full Kubernetes API (via vcluster) or to provide namespaced access within a shared cluster. Within these environments, teams can further personalise their Datalabs by installing additional tools, libraries, or configurations into their persistent workspace.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>The Workspace Building Block integrates several core components:</p> <ul> <li> <p>Workspace API and UI   Orchestrate storage, runtime, and tooling resources via a unified REST API by managing the underlying Kubernetes Custom Resources (CRs).</p> </li> <li> <p>Storage Controller (<code>provider-storage</code>)   A Kubernetes Custom Resource responsible for creating and managing S3-compatible buckets (e.g., MinIO, AWS S3, or OTC OBS).</p> </li> <li> <p>Datalab Controller (<code>provider-datalab</code>)   A Kubernetes Custom Resource used to deploy persistent VSCode-based environments with direct object-storage access \u2014 either directly on Kubernetes or within a vCluster \u2014 preconfigured with essential services and tools.</p> </li> <li> <p>Identity &amp; Access (Keycloak)   Manages user and team identities, enabling role-based access control and granting permissions to specific Datalabs and storage resources.</p> </li> </ul>"},{"location":"design/architecture/","title":"Architecture","text":"<p>The Workspace Building Block (BB) provides a cohesive and modular environment that unifies compute, storage, and tooling into a single declaratively managed system. Its architecture is Kubernetes-native, built upon Crossplane, which serves as the foundation for modeling and orchestrating complex service dependencies using Custom Resource Definitions (CRDs) and Compositions.  </p>"},{"location":"design/architecture/#architectural-composition","title":"Architectural Composition","text":"<p>At its core, the Workspace BB is composed of three tightly integrated subsystems:</p> <p>Storage Management \u2014 implemented through the provider-storage abstraction, which provisions and manages object storage across multiple backends such as MinIO, AWS S3, and OTC OBS.  </p> <ul> <li>Buckets are represented as Kubernetes CRs (<code>Storage</code> objects).  </li> <li>The reconciliation process automatically provisions S3 buckets, configures access policies (e.g. read-write grants), and returns endpoint and credential data to dependent components such as datalabs or applications.  </li> <li>Credentials are securely propagated into runtime environments via Kubernetes secrets or environment variables.  </li> <li>The system supports both local and external S3-compatible endpoints, enabling hybrid storage configurations.</li> </ul> <p>Runtime Management \u2014 handled by the provider-datalab, which builds on the educates.dev project to create isolated compute environments either as Kubernetes namespaces or as vClusters for enhanced multi-tenancy and isolation.  </p> <ul> <li>Each datalab hosts one or more user-facing applications such as VSCode Server or web-based terminal and browser interfaces.  </li> <li>Datalabs automatically connect to workspace-specific storage, mounting buckets via CSI Rclone into the file system and preloading common command-line tools (<code>awscli</code>, <code>rclone</code>, <code>boto3</code>).  </li> <li>This design ensures immediate, persistent, and secure access to datasets \u2014 supporting both exploratory workflows and automated pipelines.  </li> <li>Users can extend Datalabs by deploying additional Kubernetes-native services such as databases, dashboards, or custom data processors.</li> </ul> <p>Orchestration &amp; Interaction Layer \u2014 composed of the workspace-api and the Workspace UI, which form the primary entry point for users and operators.  </p> <ul> <li>The API provides a unified REST interface that translates user actions (e.g., creating a workspace, adding buckets, or launching Datalabs) into declarative CRDs managed by Crossplane.  </li> <li>The UI presents these capabilities graphically, enabling users to create and manage storage, grant or revoke bucket access, and invite collaborators.  </li> <li>Authentication and authorization are delegated to Keycloak, ensuring that each workspace inherits consistent identity, role, and group configurations across all integrated services.</li> </ul>"},{"location":"design/architecture/#crossplane-as-the-control-plane","title":"Crossplane as the Control Plane","text":"<p>All orchestration logic is implemented declaratively through Crossplane Compositions. Each high-level abstraction \u2014 such as Workspace, Storage, or Datalab \u2014 is modeled as a Composite Resource (XR) that maps to one or more Managed Resources (MR).</p> <p>This layered model allows infrastructure and application services to be managed through the same GitOps workflow as any Kubernetes manifest:</p> <ul> <li>Providers such as provider-kubernetes, provider-helm, provider-minio, and provider-keycloak expose declarative APIs for managing external systems.  </li> <li>Compositions define how these providers interact to realize higher-level abstractions (e.g., create a bucket, generate credentials, store them as a secret, and inject them into a datalab).  </li> <li>EnvironmentConfigs define cluster-wide parameters such as ingress domains, TLS secrets, S3 endpoints, and Keycloak realms, ensuring that all Compositions operate within a consistent configuration context.</li> </ul> <p>Each workspace is therefore not a fixed allocation but a composable graph of managed resources \u2014 continuously synchronized through Crossplane\u2019s reconciliation loop to ensure desired state alignment.</p>"},{"location":"design/architecture/#runtime-and-storage-integration","title":"Runtime and Storage Integration","text":"<p>A key design goal of the Workspace BB is data proximity and transparent access. Every datalab automatically receives credentials for the storage resources within its workspace scope. Using CSI Rclone, these buckets are mounted as file systems inside the containerized environment, providing familiar navigation and manipulation of data.  </p> <p>The integrated file browser in the Datalab UI allows:</p> <ul> <li>Hierarchical navigation of storage buckets  </li> <li>Preview of text, imagery, and EO product assets  </li> <li>Instant data sharing via presigned URLs  </li> </ul> <p>By exposing data through both object APIs and filesystem mounts, users can fluidly transition between interactive analysis, batch computation, and automated packaging workflows \u2014 without reconfiguring credentials or access paths.</p>"},{"location":"design/architecture/#iam-and-policy-enforcement","title":"IAM and Policy Enforcement","text":"<p>Identity and Access Management (IAM) is centralized through Keycloak, ensuring consistent user identity and access policies across the entire platform. Crossplane\u2019s provider-keycloak enables the declarative management of users, groups, clients, and roles \u2014 functionality dynamically leveraged by the provider-datalab compositions to associate these entities with specific workspaces and runtime environments.  </p> <p>This approach ensures that collaboration is securely limited to authorized users within designated workspaces, maintaining both security and compliance across all deployments. It also lays the groundwork for future extensions, enabling more granular role-based access control across services and datasets.</p>"},{"location":"design/architecture/#declarative-workflow","title":"Declarative Workflow","text":"<p>The entire system operates on declarative state entities managed through Crossplane compositions. All resources exist natively within Kubernetes and are continuously reconciled, ensuring complete traceability, consistency, and reproducibility across all components.  </p> <p>Storage, runtime, and IAM elements are defined as Kubernetes manifests and can be orchestrated either by the workspace-api or deployed manually \u2014 for instance via direct API calls or GitOps tools such as Flux or ArgoCD. This approach provides full flexibility in how the platform is operated, including hybrid modes where certain entities are managed declaratively through GitOps while others are created dynamically via API interactions. This model is particularly advantageous for static or example setups, which can be easily bootstrapped and maintained using the same declarative principles.</p>"},{"location":"design/architecture/#motivation-for-switch-from-v1-to-v2","title":"Motivation for Switch from v1 to v2","text":"<p>The initial v1 implementation of the Workspace introduced a functional but rigid design that limited operator flexibility, scalability, and usability in collaborative contexts.  </p> <ul> <li>Each workspace included exactly one bucket and a fixed set of services, preventing customization for specific projects or workloads.  </li> <li>A workspace was implicitly tied to a single user through a naming convention \u2014 a team concept did not exist.  </li> <li>Service deployment relied on imperative API calls, making upgrades, versioning, and selective redeployment difficult to automate.  </li> <li>Infrastructure provisioning (e.g., buckets, IAM policies) required operator-specific logic and custom webhook extensions.  </li> <li>Multi-tenancy was limited to namespace isolation, which complicated CRD versioning, RBAC scoping, and cluster-wide policy management.  </li> </ul> <p>The v2 model resolves these limitations through a declarative workflow based on composable pipelines and continuous reconciliation.  It introduces flexible team membership, optional vCluster integration for enhanced multi-tenancy, and a unified GitOps-compatible model \u2014 allowing operators and users alike to declaratively define and evolve workspaces, services, and access policies with full transparency and control.</p>"},{"location":"design/iam-integration/","title":"IAM Integration","text":"<p>The Workspace Building Block (BB) provides a unified and secure access model that connects Keycloak-based identity management, object storage authorization, and ingress-level enforcement into one cohesive system.  It ensures that both users and workloads can seamlessly and securely access workspace resources \u2014 including Datalabs, storage buckets, and shared services \u2014 using centrally managed identities and declarative policies.</p>"},{"location":"design/iam-integration/#key-principles","title":"Key Principles","text":"<ul> <li> <p>Full Keycloak Integration:   Each workspace is represented as a first-class entity within Keycloak.   Workspace-specific Keycloak clients, roles, and groups are created automatically during provisioning, ensuring end-to-end access management for the Workspace UI, Datalab, and APIs.</p> </li> <li> <p>Automated Membership and Role Management:   Adding or removing workspace members automatically updates Keycloak group membership and role assignments. These changes take effect immediately, ensuring that workspace access reflects the current membership state at all times.</p> </li> <li> <p>Unified Principal for Storage Access:   Each workspace has a corresponding storage principal (e.g., S3 user or service account) in the configured backend (MinIO, AWS S3, or OTC OBS).   This principal owns the workspace\u2019s buckets, and all workspace members share its credentials \u2014 securely injected into their runtime environments through Kubernetes Secrets.   This design ensures that all users and workloads within a workspace operate under the same \u201cworkspace identity\u201d when accessing object storage.</p> </li> </ul>"},{"location":"design/iam-integration/#workspace-level-iam-entities","title":"Workspace-Level IAM Entities","text":"<p>When a workspace is provisioned, the following Keycloak entities are automatically created and configured:</p> <p>Keycloak Client </p> <ul> <li>Created per workspace (e.g., <code>ws-alice</code>)  </li> <li>Used for OpenID Connect authentication at ingress level  </li> <li>Defines a role (e.g., <code>ws_access</code>) used to authorize requests to the workspace endpoints  </li> </ul> <p>Keycloak Group </p> <ul> <li>Mirrors the workspace name (e.g., <code>ws-alice</code>)  </li> <li>Group membership defines access to the Workspace UI and Datalab  </li> <li>The workspace owner is automatically added to the group as the initial member  </li> </ul> <p>Adding or removing users from the workspace group dynamically updates their access to the UI, API, and Datalab without manual operator intervention.</p>"},{"location":"design/iam-integration/#storage-level-iam-entities","title":"Storage-Level IAM Entities","text":"<p>A dedicated storage principal is created in the configured storage backend:</p> <ul> <li>This principal defines the workspace\u2019s identity at the storage layer.  </li> <li>Buckets are created under this principal, with IAM policies applied automatically.  </li> <li>The credentials (access key, secret key) are propagated to the workspace through Kubernetes Secrets and mounted directly into Datalabs and workloads.  </li> </ul> <p>All workspace members use these same credentials, inheriting the principal\u2019s permissions \u2014 enabling uniform access while isolating each workspace\u2019s data from others.  </p> <p>Shared Buckets: When a bucket is shared between workspaces:</p> <ul> <li>Storage policies in the backend (MinIO, AWS S3, or OBS) are updated automatically to grant or revoke the defined access (<code>readOnly</code>, <code>readWrite</code>, <code>writeOnly</code>).  </li> <li>These policies are linked to the workspace principal of the respective workspaces.  </li> </ul>"},{"location":"design/iam-integration/#ingress-protection-and-authorization","title":"Ingress Protection and Authorization","text":"<p>Access to workspace endpoints (UI, API, and Datalab) is protected at the ingress layer following the common EOEPCA IAM concepts.  The Workspace BB leverages APISix ingress configuration combined with Open Policy Agent (OPA) policies to enforce both authentication and fine-grained authorization.  </p> <ul> <li>The APISix OIDC plugin validates user tokens via Keycloak and ensures only authenticated users can reach workspace endpoints.  </li> <li>The OPA plugin enforces declarative access rules defined in policy repositories (e.g., <code>eoepca/iam-policies</code>), controlling which users or roles can access which workspaces.  </li> <li>These policies are managed in a GitOps-compatible way, enabling version-controlled and auditable authorization configurations across environments.</li> </ul> <p>This approach seamlessly ties ingress-level access control to the same Keycloak clients and groups that govern workspace membership, ensuring uniform and transparent access enforcement.</p>"},{"location":"design/iam-integration/#credentials-management-and-future-enhancements","title":"Credentials Management and Future Enhancements","text":"<ul> <li> <p>Automatic Credential Injection:   Storage credentials are securely mounted into all Datalab sessions and workloads via Kubernetes Secrets.   Users and services can immediately access workspace buckets without manually handling access keys.</p> </li> <li> <p>Credential Rollover (Planned):   Future releases will support periodic credential rotation and lifecycle management to improve security and compliance.</p> </li> <li> <p>Vended (Time-Limited) Credentials (Planned):   Short-lived credentials will be supported to grant temporary access to shared resources \u2014 providing secure, time-bound data sharing and external system integration.</p> </li> </ul>"},{"location":"design/operational-controls/","title":"Operational Controls","text":""},{"location":"design/operational-controls/#session-mode","title":"Session Mode","text":"<p>If sessions are not generally disabled via the corresponding session mode flag, the Workspace Building Block (BB) provides isolated, user-facing compute environments (Datalabs) running on top of a shared Kubernetes host cluster.  These sessions can be either long-running or started on-demand (in so called <code>Auto</code> session mode), with background jobs automatically shutting down inactive sessions according to configurable criteria.  Both modes maintain persistent access to user files, configurations, and mounted object storage, ensuring seamless continuity between session restarts.</p> <p>Each workspace represents a logical boundary for a team or project and includes its own compute, storage, and access configuration.  While users are free to execute arbitrary code, deploy additional components through the Kubernetes API (e.g., via <code>kubectl</code>), or interact with data via networked storage and APIs, platform operators must ensure that this flexibility remains secure, resource-efficient, and compliant with operational policies.  </p> <p>Details on the underlying security model of the Datalab environment are available in the provider-datalab security documentation.</p>"},{"location":"design/operational-controls/#cluster-exposure","title":"Cluster Exposure","text":"<p>Workspaces can run either in namespace mode (shared host control plane) or in vCluster mode (isolated virtual control plane based on vCluster).</p> <ul> <li>In namespace mode, workspaces share the host Kubernetes control plane. This mode has minimal overhead and is suited for trusted environments where global RBAC and quotas can be centrally enforced.  </li> <li>In vCluster mode, each workspace runs its own k3s-based control plane (API server, controller-manager, scheduler) inside a pod on the host cluster. Users interact with the vCluster as if it were a standalone Kubernetes cluster.</li> </ul> <p>vClusters offer stronger isolation but introduce a modest baseline overhead (~300\u2013500 MiB RAM, 0.1\u20130.2 vCPU per idle control plane).</p>"},{"location":"design/operational-controls/#deployment-permissions","title":"Deployment Permissions","text":"<p>Controlled through Kubernetes RBAC and namespace-level policies. </p> <ul> <li>Operators can restrict the creation of <code>Ingress</code>, <code>ClusterRole</code>, or <code>ClusterRoleBinding</code> resources.  </li> <li>Privileged pods or the use of <code>HostPath</code> mounts can be blocked.  </li> <li>Resource consumption can be limited through <code>LimitRange</code> and <code>ResourceQuota</code> definitions on the namespace level, passed through provider-datalab configuration.</li> </ul>"},{"location":"design/operational-controls/#network-isolation","title":"Network Isolation","text":"<p>Enforced via NetworkPolicies to define allowed communication paths.  </p> <ul> <li>Outbound traffic can be limited.  </li> <li>Cross-workspace pod communication can blocked to prevent lateral movement.  </li> <li>Egress to the public internet can be disabled, except for approved external data sources.  </li> </ul>"},{"location":"design/operational-controls/#ingress-and-authentication","title":"Ingress and Authentication","text":"<p>Ingress routing should handled centrally at the host-cluster level (not within individual vClusters).  </p>"},{"location":"design/operational-controls/#data-persistence-and-storage","title":"Data Persistence and Storage","text":"<ul> <li> <p>Persistent volumes are used for:</p> </li> <li> <p>Control plane state (vCluster metadata)  </p> </li> <li>User home directories and shared data (<code>ReadWriteMany</code> PVCs)  </li> <li>Object storage integration via CSI Rclone</li> </ul>"},{"location":"getting-started/concepts/","title":"Concepts","text":"<p>This page introduces the core concepts of the Workspace platform: who uses it, what it\u2019s made of, and how collaboration actually works. At its heart, a Workspace brings together three elements \u2014 Storage, the Runtime environment (Datalab), and Tools &amp; Services \u2014 under a single identity and lifecycle.</p> <ul> <li>Storage offers S3-compatible buckets with consistent policies.  </li> <li>The Datalab provides a browser-based environment (VS Code Server, Terminal, Data Browser) wired to those buckets and credentials.  </li> <li>Tooling supplies pre-configured CLIs and SDKs combined with operator-provided and user-managed Services, so teams can work productively from day one.  </li> </ul> <p>The Workspace ties these parts together by orchestrating Crossplane Compositions, ensuring that what you run is reproducible, auditable, and easy to reason about.</p> <p>The platform serves three personas with overlapping responsibilities.</p> <p>Platform Operators stand up and maintain the system, connect identity, and enforce guardrails.  </p> <p>Workspace Managers curate the project space, invite members, organize buckets, and define how data is shared.  </p> <p>Workspace Users focus on exploration and delivery, using the Datalab and tools to turn data into results.  </p> <p>While these roles clarify intent, there is currently no strict RBAC split between \u201cmanager\u201d and \u201cuser\u201d: once assigned to a workspace, members possess the same capabilities within that environment.</p> <p>Identity and access cut across everything. Authentication is centralized in Keycloak (OIDC), and authorization is expressed through workspace membership and bucket-level policies. These bucket-level policies are deliberately simple (<code>readOnly</code>, <code>readWrite</code>, <code>writeOnly</code>) and grant flows are supported through an interactive UI. For sustained collaboration, workspaces grant each other access so foreign buckets appear alongside local ones with no extra bootstrap steps. For ad-hoc, external sharing of individual objects, users can mint pre-signed URLs directly from the lab\u2019s Data Browser.  </p> <p>The Datalab can be on-demand (auto-start/cull) for cost-effective interactivity or always-on when continuous services are required. Operators can provide shared services as the Datalab runs on the same underlying Kubernetes cluster, and users can deploy their own session-bound additional Servicers \u2014 all under the same policy umbrella.</p> <p>For deeper dives into the building blocks: storage capabilities are implemented by provider-storage, and the interactive runtime by provider-datalab. Their documentation explains the operational details, usage patterns, and guardrails you can apply in production.</p>"},{"location":"getting-started/concepts/#storage-in-practice","title":"Storage in Practice","text":"<p>The object storage layer is built and abstracted to allow both operators and users to deploy across different backends depending on needs, constraints, and budgets \u2014 providing a unified model for bucket access and sharing.  </p> <p>It gives end users a simple way to request and share S3 buckets, and it gives operators a consistent control plane to enforce policies across multiple backends such as MinIO, AWS S3, OTC OBS, and others.  </p>"},{"location":"getting-started/concepts/#example-flow-cross-workspace-collaboration","title":"Example Flow: Cross-workspace collaboration","text":"<p>Users from Alice\u2019s workspace (<code>ws-alice</code>) want to access data stored in Eric\u2019s shared workspace (<code>ws-eric-shared</code>). They therefore request access to the relevant bucket, and a member of Eric\u2019s team can approve or deny the request, i.e. grant access via simple click when collaboration is desired.</p> <p>Result: Alice\u2019s team can directly browse and read the curated objects from Eric\u2019s shared bucket without duplicating data. If the request is denied, the bucket remains inaccessible, ensuring that all data exchange is intentional and auditable.</p>"},{"location":"getting-started/concepts/#example-flow-ad-hoc-sharing-with-pre-signed-urls","title":"Example Flow: Ad-hoc sharing with pre-signed URLs","text":"<p>Eric holds interesting VHR data in his bucket and a colleague asks him to share a specific file for review.  He previews the data in the Data Browser and generates a pre-signed URL directly from the Datalab, which produces a time-limited link. The colleague can then access the object directly without needing a Workspace account or any prior setup.</p> <p>Result: The file becomes temporarily accessible to anyone holding the link until it expires. This lightweight sharing method is ideal for quick, one-off reviews or data exchanges outside the workspace, while long-term collaboration should rely on workspace-to-workspace access grants.</p> <p>For further information on specific aspects of the Storage Layer:</p> <ul> <li>Usage Concepts \u2014 explains how users can create, view, and manage S3 buckets within their workspace, how storage requests are processed, and how shared buckets appear across workspaces through declarative grants.  </li> <li>Permissions and Access Control \u2014 describes how workspace-to-workspace access is managed, how readOnly, readWrite, and writeOnly permissions are enforced, and how users can grant or revoke access directly through the Workspace UI.  </li> </ul>"},{"location":"getting-started/concepts/#datalab-in-practice","title":"Datalab in Practice","text":"<p>The Datalab is the powerhouse of the workspace experience \u2014 an interactive runtime that connects compute, storage, and services. It can operate in on-demand mode, where sessions start automatically when needed and shut down when idle, or in always-on mode for continuous collaboration or long-running services. Users can also launch auxiliary services bound to their session, such as dashboards, notebooks, or backend APIs, while operators can provide persistent shared services for teams. All Datalab sessions respect workspace policies and quotas, inheriting network rules, resource limits, and access control. Buckets are mounted or accessed via SDKs and command-line tools, while the integrated object browser allows quick previews and pre-signed URL generation.</p>"},{"location":"getting-started/concepts/#example-flow-deploying-mlflow-for-experiment-tracking","title":"Example Flow: Deploying MLflow for Experiment Tracking","text":"<p>Alice wants to train a machine learning model in a Jupyter Notebook within her workspace. To ensure the experiment is tracked and reproducible, she deploys MLflow directly from the Datalab environment. This allows her to monitor metrics, parameters, and results interactively as the training progresses.</p> <p>Result: The MLflow UI becomes available directly from the Datalab, providing a seamless and interactive experience for experiment tracking and visualization \u2014 all within the same authenticated workspace environment.</p> <p>For further information on specific aspects of the Datalab:</p> <ul> <li>Usage Concepts \u2014 explains how users can start, stop, and reconnect Datalab sessions across workspaces. It also outlines persistence options, data retention behavior, and the lifecycle of ephemeral versus persistent sessions.  </li> <li>Additional Services \u2014 describes how to extend a Datalab session with additional services, such as dashboards, APIs, or other runtime components that can be attached to a user session or shared workspace.  </li> <li>Security and Constraints \u2014 discusses how authentication, network isolation, and configuration settings are applied in Datalab environments, as well as how quotas and resource limits ensure secure multi-tenant operation.</li> </ul>"},{"location":"getting-started/operator-view/","title":"Operator View","text":"In\u00a0[1]: Copied! <pre>import jwt\nimport requests\nimport datetime\nimport time\nimport os\nfrom dotenv import load_dotenv\nimport urllib3\nfrom pathlib import Path\n\nurllib3.disable_warnings()\n\nroot = Path.cwd()\nwhile not (root / \"eoepca-demo.env\").exists():\n    root = root.parent\n\nload_dotenv(root / \"eoepca-demo.env\")\n</pre> import jwt import requests import datetime import time import os from dotenv import load_dotenv import urllib3 from pathlib import Path  urllib3.disable_warnings()  root = Path.cwd() while not (root / \"eoepca-demo.env\").exists():     root = root.parent  load_dotenv(root / \"eoepca-demo.env\") Out[1]: <pre>True</pre> In\u00a0[2]: Copied! <pre>realm = os.getenv(\"REALM\")\nbase_domain = os.getenv(\"BASE_DOMAIN\")\nkeycloak_endpoint = os.getenv(\"KEYCLOAK_ENDPOINT\")\ndefault_scope = os.getenv(\"DEFAULT_SCOPE\")\nclient_id = os.getenv(\"CLIENT_ID\")\nclient_secret = os.getenv(\"CLIENT_SECRET\")\n\nissuer = f\"{keycloak_endpoint}/realms/{realm}\"\ntoken_endpoint = f\"{issuer}/protocol/openid-connect/token\"\ndevice_authorization_endpoint = f\"{issuer}/protocol/openid-connect/auth/device\"\nuserinfo_endpoint = f\"{issuer}/protocol/openid-connect/userinfo\"\n\nworkspace_api_endpoint = f'https://workspace-api.{base_domain}/workspaces'\nworkspace_api_endpoint\n</pre> realm = os.getenv(\"REALM\") base_domain = os.getenv(\"BASE_DOMAIN\") keycloak_endpoint = os.getenv(\"KEYCLOAK_ENDPOINT\") default_scope = os.getenv(\"DEFAULT_SCOPE\") client_id = os.getenv(\"CLIENT_ID\") client_secret = os.getenv(\"CLIENT_SECRET\")  issuer = f\"{keycloak_endpoint}/realms/{realm}\" token_endpoint = f\"{issuer}/protocol/openid-connect/token\" device_authorization_endpoint = f\"{issuer}/protocol/openid-connect/auth/device\" userinfo_endpoint = f\"{issuer}/protocol/openid-connect/userinfo\"  workspace_api_endpoint = f'https://workspace-api.{base_domain}/workspaces' workspace_api_endpoint Out[2]: <pre>'https://workspace-api.develop.eoepca.org/workspaces'</pre> In\u00a0[3]: Copied! <pre># Helper function to get token via device flow\ndef token_via_device_flow(\n    client_id: str,\n    client_secret: str | None = None,\n    scope: str = \"openid profile email\",\n    poll_interval: int | None = None,\n    timeout: int = 600\n):\n    da_resp = requests.post(\n        device_authorization_endpoint,\n        headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        data={\n            \"client_id\": client_id,\n            **({\"client_secret\": client_secret} if client_secret else {}),\n            \"scope\": scope,\n        },\n        timeout=30,\n    )\n    da_resp.raise_for_status()\n    da = da_resp.json()\n\n    device_code = da[\"device_code\"]\n    user_code = da[\"user_code\"]\n    verification_uri = da[\"verification_uri\"]\n    verification_uri_complete = da.get(\"verification_uri_complete\")\n    interval = poll_interval or da.get(\"interval\", 5)\n    expires_in = da.get(\"expires_in\", timeout)\n\n    print(f\"Open: {verification_uri}\")\n    print(f\"Enter code: {user_code}\")\n    if verification_uri_complete:\n        print(f\"(Or open directly): {verification_uri_complete}\")\n\n    start = time.time()\n    data = {\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n        \"device_code\": device_code,\n        \"client_id\": client_id,\n        **({\"client_secret\": client_secret} if client_secret else {}),\n    }\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n\n    while True:\n        if time.time() - start &gt; min(timeout, expires_in):\n            raise TimeoutError(\"Device authorization timed out.\")\n        resp = requests.post(token_endpoint, headers=headers, data=data, timeout=30)\n        if resp.ok:\n            tok = resp.json()\n            break\n        try:\n            err = resp.json().get(\"error\")\n        except Exception:\n            resp.raise_for_status()\n        if err == \"authorization_pending\":\n            time.sleep(interval)\n            continue\n        elif err == \"slow_down\":\n            interval += 5\n            time.sleep(interval)\n            continue\n        elif err in (\"access_denied\", \"expired_token\", \"invalid_grant\"):\n            raise RuntimeError(f\"Device flow failed: {err}\")\n        else:\n            resp.raise_for_status()\n\n    access_token = tok.get(\"access_token\")\n    decoded = jwt.decode(access_token, options={\"verify_signature\": False, \"verify_aud\": False})\n\n    print(\"\\n--- Decoded Access Token ---\")\n    for k, v in decoded.items():\n        print(f\"{k}: {v}\")\n\n    ra = decoded.get(\"resource_access\", {}) or {}\n    roles_wsapi = ra.get(client_id, {}).get(\"roles\", []) or []\n    is_operator = \"admin\" in roles_wsapi\n\n    workspaces = []\n    for client, meta in ra.items():\n        roles = (meta or {}).get(\"roles\", []) or []\n        if client.startswith(\"ws-\") and \"ws_access\" in roles:\n            workspaces.append(client)\n\n    print(\"\\n--- Access Summary ---\")\n    if is_operator:\n        print(\"user is OPERATOR\")\n    elif workspaces:\n        print(f\"user has WORKSPACE ACCESS: {', '.join(sorted(workspaces))}\")\n    else:\n        print(\"user is neither OPERATOR nor has WORKSPACE ACCESS\")\n\n    return access_token, is_operator, sorted(workspaces)\n</pre> # Helper function to get token via device flow def token_via_device_flow(     client_id: str,     client_secret: str | None = None,     scope: str = \"openid profile email\",     poll_interval: int | None = None,     timeout: int = 600 ):     da_resp = requests.post(         device_authorization_endpoint,         headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},         data={             \"client_id\": client_id,             **({\"client_secret\": client_secret} if client_secret else {}),             \"scope\": scope,         },         timeout=30,     )     da_resp.raise_for_status()     da = da_resp.json()      device_code = da[\"device_code\"]     user_code = da[\"user_code\"]     verification_uri = da[\"verification_uri\"]     verification_uri_complete = da.get(\"verification_uri_complete\")     interval = poll_interval or da.get(\"interval\", 5)     expires_in = da.get(\"expires_in\", timeout)      print(f\"Open: {verification_uri}\")     print(f\"Enter code: {user_code}\")     if verification_uri_complete:         print(f\"(Or open directly): {verification_uri_complete}\")      start = time.time()     data = {         \"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",         \"device_code\": device_code,         \"client_id\": client_id,         **({\"client_secret\": client_secret} if client_secret else {}),     }     headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}      while True:         if time.time() - start &gt; min(timeout, expires_in):             raise TimeoutError(\"Device authorization timed out.\")         resp = requests.post(token_endpoint, headers=headers, data=data, timeout=30)         if resp.ok:             tok = resp.json()             break         try:             err = resp.json().get(\"error\")         except Exception:             resp.raise_for_status()         if err == \"authorization_pending\":             time.sleep(interval)             continue         elif err == \"slow_down\":             interval += 5             time.sleep(interval)             continue         elif err in (\"access_denied\", \"expired_token\", \"invalid_grant\"):             raise RuntimeError(f\"Device flow failed: {err}\")         else:             resp.raise_for_status()      access_token = tok.get(\"access_token\")     decoded = jwt.decode(access_token, options={\"verify_signature\": False, \"verify_aud\": False})      print(\"\\n--- Decoded Access Token ---\")     for k, v in decoded.items():         print(f\"{k}: {v}\")      ra = decoded.get(\"resource_access\", {}) or {}     roles_wsapi = ra.get(client_id, {}).get(\"roles\", []) or []     is_operator = \"admin\" in roles_wsapi      workspaces = []     for client, meta in ra.items():         roles = (meta or {}).get(\"roles\", []) or []         if client.startswith(\"ws-\") and \"ws_access\" in roles:             workspaces.append(client)      print(\"\\n--- Access Summary ---\")     if is_operator:         print(\"user is OPERATOR\")     elif workspaces:         print(f\"user has WORKSPACE ACCESS: {', '.join(sorted(workspaces))}\")     else:         print(\"user is neither OPERATOR nor has WORKSPACE ACCESS\")      return access_token, is_operator, sorted(workspaces)  In\u00a0[4]: Copied! <pre># Retrieve token via device flow - use `oscar``\ntoken, is_operator, workspaces = token_via_device_flow(client_id=client_id, client_secret=client_secret)\n</pre> # Retrieve token via device flow - use `oscar`` token, is_operator, workspaces = token_via_device_flow(client_id=client_id, client_secret=client_secret) <pre>Open: https://iam-auth.develop.eoepca.org/realms/eoepca/device\nEnter code: SUNG-RIPV\n(Or open directly): https://iam-auth.develop.eoepca.org/realms/eoepca/device?user_code=SUNG-RIPV\n\n--- Decoded Access Token ---\nexp: 1768815636\niat: 1768815336\nauth_time: 1768815333\njti: 12a8c26e-488d-413c-9f74-7aae0a4281fe\niss: https://iam-auth.develop.eoepca.org/realms/eoepca\naud: account\nsub: 4357d7f7-b706-412d-b2e7-62199aa48db2\ntyp: Bearer\nazp: workspace-api\nsid: a6b5aa12-2137-430c-a7f5-d7a64256b939\nacr: 1\nallowed-origins: ['*']\nrealm_access: {'roles': ['offline_access', 'default-roles-eoepca', 'uma_authorization']}\nresource_access: {'workspace-api': {'roles': ['admin']}, 'account': {'roles': ['manage-account', 'view-consent', 'manage-account-links', 'manage-consent', 'view-profile']}}\nscope: openid profile email\nemail_verified: True\nname: Oscar O\ngroups: ['/workspace-admin']\npreferred_username: oscar\ngiven_name: Oscar\nfamily_name: O\nemail: oscar@eoepca.org\n\n--- Access Summary ---\nuser is OPERATOR\n</pre> In\u00a0[5]: Copied! <pre>assert is_operator # user must be operator to run this notebook\n</pre> assert is_operator # user must be operator to run this notebook In\u00a0[6]: Copied! <pre># Create workspace via API\nowner = \"test1\"\npreferred_name = f\"{owner}-{datetime.datetime.now().timestamp():.0f}\"\npreferred_name\nresponse = requests.post(\n    workspace_api_endpoint,\n    headers={\n        'Authorization': 'Bearer ' + token\n    },\n    json={\n        \"preferred_name\": preferred_name,\n        \"default_owner\": owner\n    }\n)\nresponse.raise_for_status()\nactual_workspace_name = response.json()['name']\nws_name = response.json()[\"name\"]\nprint(f\"\u2705 triggered workspace creation '{ws_name}' for '{owner}'\")\n</pre> # Create workspace via API owner = \"test1\" preferred_name = f\"{owner}-{datetime.datetime.now().timestamp():.0f}\" preferred_name response = requests.post(     workspace_api_endpoint,     headers={         'Authorization': 'Bearer ' + token     },     json={         \"preferred_name\": preferred_name,         \"default_owner\": owner     } ) response.raise_for_status() actual_workspace_name = response.json()['name'] ws_name = response.json()[\"name\"] print(f\"\u2705 triggered workspace creation '{ws_name}' for '{owner}'\") <pre>\u2705 triggered workspace creation 'ws-test1-1768815339' for 'test1'\n</pre> In\u00a0[7]: Copied! <pre># Helper function to take screenshot using playwright\nfrom playwright.async_api import async_playwright\nfrom IPython.display import Image, display\n\nasync def snap_with_bearer_full(url: str, access_token: str, widen=True, fix_scroll=False):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        ctx = await browser.new_context(\n            extra_http_headers={\"Authorization\": f\"Bearer {access_token}\"},\n            viewport={\"width\": 1920, \"height\": 1080} if widen else None,\n            device_scale_factor=2  # crisp\n        )\n        page = await ctx.new_page()\n        await page.goto(url, wait_until=\"networkidle\")\n\n        if fix_scroll:\n            await page.add_style_tag(content=\"\"\"\n                html, body { height: auto !important; }\n                [style*=\"overflow\"], .scroll, .scrollable, .overflow-auto, .overflow-y-auto {\n                    overflow: visible !important; max-height: none !important; height: auto !important;\n                }\n            \"\"\")\n\n        img = await page.screenshot(full_page=True)  # entire page height\n        await browser.close()\n        display(Image(data=img))\n</pre> # Helper function to take screenshot using playwright from playwright.async_api import async_playwright from IPython.display import Image, display  async def snap_with_bearer_full(url: str, access_token: str, widen=True, fix_scroll=False):     async with async_playwright() as p:         browser = await p.chromium.launch(headless=True)         ctx = await browser.new_context(             extra_http_headers={\"Authorization\": f\"Bearer {access_token}\"},             viewport={\"width\": 1920, \"height\": 1080} if widen else None,             device_scale_factor=2  # crisp         )         page = await ctx.new_page()         await page.goto(url, wait_until=\"networkidle\")          if fix_scroll:             await page.add_style_tag(content=\"\"\"                 html, body { height: auto !important; }                 [style*=\"overflow\"], .scroll, .scrollable, .overflow-auto, .overflow-y-auto {                     overflow: visible !important; max-height: none !important; height: auto !important;                 }             \"\"\")          img = await page.screenshot(full_page=True)  # entire page height         await browser.close()         display(Image(data=img)) In\u00a0[8]: Copied! <pre># Check workspace creation status via API\nheaders = {\n    'Authorization': 'bearer ' + token\n}\nurl = f\"{workspace_api_endpoint}/{ws_name}\"\nwhile True:\n    print(f\"HTTP GET {url}\")\n    response = requests.get(url, headers=headers)\n    print(response)\n    if response.status_code == 200:\n        try:\n            workspace_data = response.json()\n            print(workspace_data.get(\"status\"))\n            if workspace_data.get(\"status\") == \"ready\":\n                print(workspace_data)\n                break\n        except ValueError:\n            print(\"not ready yet\")\n\n    print(\"...\")\n    time.sleep(10)\n</pre> # Check workspace creation status via API headers = {     'Authorization': 'bearer ' + token } url = f\"{workspace_api_endpoint}/{ws_name}\" while True:     print(f\"HTTP GET {url}\")     response = requests.get(url, headers=headers)     print(response)     if response.status_code == 200:         try:             workspace_data = response.json()             print(workspace_data.get(\"status\"))             if workspace_data.get(\"status\") == \"ready\":                 print(workspace_data)                 break         except ValueError:             print(\"not ready yet\")      print(\"...\")     time.sleep(10) <pre>HTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [500]&gt;\n...\nHTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [500]&gt;\n...\nHTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [500]&gt;\n...\nHTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [500]&gt;\n...\nHTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [500]&gt;\n...\nHTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [500]&gt;\n...\nHTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-test1-1768815339\n&lt;Response [200]&gt;\nready\n{'name': 'ws-test1-1768815339', 'creation_timestamp': '2026-01-19T09:35:39Z', 'version': '1214195355', 'status': 'ready', 'storage': {'buckets': [{'name': 'ws-test1-1768815339', 'discoverable': True}], 'credentials': {'bucketname': 'ws-test1-1768815339', 'access': 'ws-test1-1768815339-1', 'secret': 'kbtqOeVVWuJLLLeExJGYVytzSv5bOAUFrbqv4dMsFER1iu4QgXdpjIlsbHscM6DE', 'endpoint': 'https://minio.develop.eoepca.org', 'region': 'eoepca-demo'}, 'bucket_access_requests': [{'workspace': 'ws-test1-1768815339', 'bucket': 'ws-alice', 'permission': 'ReadWrite'}, {'workspace': 'ws-test1-1768815339', 'bucket': 'ws-alice-2', 'permission': 'ReadWrite'}, {'workspace': 'ws-test1-1768815339', 'bucket': 'ws-alice-3', 'permission': 'ReadWrite'}, {'workspace': 'ws-test1-1768815339', 'bucket': 'ws-bob', 'permission': 'ReadWrite'}, {'workspace': 'ws-test1-1768815339', 'bucket': 'ws-eric', 'permission': 'ReadWrite'}, {'workspace': 'ws-test1-1768815339', 'bucket': 'ws-eric-shared', 'permission': 'ReadWrite'}]}, 'datalab': {'memberships': [{'member': 'test1', 'role': 'owner', 'creation_timestamp': '2026-01-19T09:35:39Z'}], 'databases': []}, 'user': {'name': 'oscar', 'permissions': ['MANAGE_BUCKETS', 'MANAGE_DATABASES', 'MANAGE_MEMBERS', 'VIEW_BUCKETS', 'VIEW_BUCKET_CREDENTIALS', 'VIEW_DATABASES', 'VIEW_MEMBERS']}}\n</pre> <p>\u26a0\ufe0f Important: As soon as the workspace is ready, this URL above is all the information you, as the operator, need to share with the actual owner of the workspace!</p> In\u00a0[9]: Copied! <pre># Dashboard screen\nawait snap_with_bearer_full(url, token)\n</pre> # Dashboard screen await snap_with_bearer_full(url, token) In\u00a0[10]: Copied! <pre># Management screen\nawait snap_with_bearer_full(f\"{url}#/home/management\", token)\n</pre> # Management screen await snap_with_bearer_full(f\"{url}#/home/management\", token) In\u00a0[11]: Copied! <pre># Delete workspace via API\nheaders = {\n    'Authorization': 'bearer ' + token\n}\nresponse = requests.delete(\n    f\"{workspace_api_endpoint}/{ws_name}\",\n    headers=headers\n)\nresponse.raise_for_status()\nprint(f\"deleted workspace '{ws_name}'\")\n</pre> # Delete workspace via API headers = {     'Authorization': 'bearer ' + token } response = requests.delete(     f\"{workspace_api_endpoint}/{ws_name}\",     headers=headers ) response.raise_for_status() print(f\"deleted workspace '{ws_name}'\") <pre>deleted workspace 'ws-test1-1768815339'\n</pre> In\u00a0[12]: Copied! <pre># Verify workspace got delete\nawait snap_with_bearer_full(f\"{url}\", token)\n</pre> # Verify workspace got delete await snap_with_bearer_full(f\"{url}\", token)"},{"location":"getting-started/operator-view/#operator-view","title":"Operator View\u00b6","text":"<p>This notebook demonstrates how an operator can create or delete a workspace for a user through simple HTTP requests.</p> <p>Note: To perform the actions in this notebook, the operator must be properly authenticated as a user with workspace administration permissions - specifically, the <code>admin</code> role assigned to the <code>workspace-api</code> client. User <code>oscar</code> has the required permissions on the EOEPCA demo system.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>This guide walks through a fictive end-to-end scenario to be executed on the <code>eoepca-demo</code> Kubernetes cluster to demonstrate the capabilities of the Workspace Building Block (BB). It highlights the roles of different personas \u2014 operator, manager, and user \u2014 and covers workspace creation, member management, data sharing on Object Storage, and collaborative use of the runtime in so called Datalabs.</p>"},{"location":"getting-started/quickstart/#environment-assumptions","title":"Environment Assumptions","text":"<ul> <li>Object Storage: MinIO (S3 compatible). Alternatives (AWS S3, OTC OBS, \u2026) work the same.</li> <li>Session Mode: <code>Auto</code> (on-demand start/stop). <code>AlwaysOn</code> also works for this guide.</li> <li>Isolation Mode: <code>useVcluster=true</code> (workspace-scoped vcluster). Namespace mode would also work here.</li> <li>Users in Keycloak: <code>alice</code>, <code>bob</code>, <code>eric</code>, <code>frank</code>; platform operator <code>oscar</code> (has <code>admin</code> role on the Workspace client).</li> <li>Existing Workspaces: <code>ws-alice</code>, <code>ws-bob</code>, <code>ws-eric</code>.</li> </ul>"},{"location":"getting-started/quickstart/#scenario","title":"Scenario","text":"<p>Frank is prototyping a data-driven AI workflow. He needs:</p> <ul> <li>A workspace with a VS Code like runtime to develop and and buckets for storage.</li> <li>Alice as a collaborator (co-development).</li> <li>Read access to Bob\u2019s shared reference data.</li> <li>A way to let Eric stage subsets of large ground-truth data into Frank\u2019s workspace.</li> <li>MLflow for experiment tracking, with model artifacts stored in Frank\u2019s bucket.</li> <li>A clear separation of storage buckets, each potentially holding data ranging from gigabytes to terabytes.</li> </ul> <p>Personas:</p> <ul> <li>Oscar \u2014 platform operator, provisions the workspace.</li> <li>Frank \u2014 workspace owner.</li> <li>Alice \u2014 collaborator in Frank\u2019s workspace.</li> <li>Eric \u2014 provides a shared reference bucket.</li> <li>Bob \u2014 curates large datasets and stages subsets for Frank. He also uses the managed database instance of his workspace to keep track of what has been prepared and delivered.</li> </ul>"},{"location":"getting-started/quickstart/#1-oscar-creating-a-workspace-for-frank","title":"1) Oscar: Creating a Workspace for Frank","text":"<p>Oscar sets up a new workspace for Frank using an HTTP API\u2013driven approach, enabling seamless integration into the existing workflows of the platform operations team.</p> <p>The same setup can alternatively be created directly via the Kubernetes API (e.g., using <code>kubectl</code>).</p> <p>After authenticating (e.g., through the OAuth2 Device Code Flow to obtain a <code>TOKEN</code>; see Operator View), Oscar initiates the workspace creation request:</p> <pre><code>curl -X POST \"https://workspace-api.develop.eoepca.org/workspaces\" \\\n  -H \"Authorization: Bearer ${TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"preferred_name\": \"frank\",\n    \"default_owner\": \"frank\"\n  }'\n</code></pre> <p>Expected result: The API returns a workspace URL such as:</p> <pre><code>https://workspace-api.develop.eoepca.org/workspaces/ws-frank\n</code></pre> <p>(where <code>ws-</code> is the configured prefix for workspace names).</p> <p>Oscar then shares this URL with Frank.</p>"},{"location":"getting-started/quickstart/#2-frank-open-the-workspace-and-verify-access","title":"2) Frank: Open the Workspace and verify access","text":"<ol> <li>Frank opens the URL in a browser and logs in.  </li> <li>The Workspace Dashboard displays:</li> </ol> <ul> <li>Workspace details (<code>ws-frank</code>)</li> <li>Default S3 bucket credentials (for <code>ws-frank</code>)</li> <li>Links to the Datalab and management pages`</li> </ul>"},{"location":"getting-started/quickstart/#3-frank-start-the-datalab-and-explore-the-environment","title":"3) Frank: Start the Datalab and explore the environment","text":"<ol> <li>Click Open Datalab.  </li> <li>After the session starts, Frank sees a familiar VS Code interface: Terminal, Editor, and a Data tab.</li> </ol> <p>Verify S3 connectivity in Datalab terminal:</p> <pre><code>aws s3 ls\naws s3 ls s3://ws-frank --recursive\n</code></pre> <p></p> <p></p>"},{"location":"getting-started/quickstart/#4-frank-use-mlflow-in-the-datalab-environment-as-additional-service","title":"4) Frank: Use MLflow in the Datalab environment as Additional Service","text":"<p>Frank wants MLflow for experiment tracking (artifacts to go to <code>ws-frank</code>).</p> <p>Clone an example project: <pre><code>git clone https://github.com/mlflow/mlflow-example\ncd mlflow-example\n</code></pre></p> <p></p> <p>Run example project with local MLflow:</p> <p>Note: Frank prefers to use <code>uv</code> over conda so he has to convert the environment file</p> <pre><code>yq eval -o=json '.dependencies[]' conda.yaml | jq -r '\n  if type==\"string\" then .\n  elif has(\"pip\") then .pip[]\n  else empty end\n' &gt; requirements.txt\n\nuv venv\nuv pip install -r requirements.txt\nuv run python train.py\n</code></pre> <p></p> <p>That worked successfully, but so far MLflow is only running in a local development shell. Next, it will be deployed as an additional service within the workspace.</p> <p>Deploy MLflow Server and Run Example Project Again</p> <p>Since <code>kubectl</code> is already preinstalled and configured for the connected Kubernetes cluster, we can simply apply the manifest (for example, the one provided in the documentation).</p> <p></p> <p>Next, port-forward the MLflow service so that it appears in the Ports tab, allowing direct access to the MLflow UI in the browser.</p> <pre><code>kubectl port-forward svc/mlflow 5000:5000\n</code></pre> <p></p> <p></p> <p>Now, point to the newly deployed MLflow server and run the example project again:</p> <pre><code>export MLFLOW_TRACKING_URI=\"http://localhost:5000\"\nexport MLFLOW_EXPERIMENT_NAME=\"frank-experiment-1\"\nuv run python train.py\n</code></pre> <p></p> <p></p> <p>The trained model is  stored as an MLflow artifact in a bucket within Frank\u2019s workspace. Frank can also navigate to the Data tab of the Datalab environment, locate the artifacts, and share specific files \u2014 for example, the <code>.pkl</code> file with the model \u2014 via a presigned URL.  This allows others to download the (potentially large) ML model directly from the bucket using a temporary access link, without requiring permanent credentials.</p> <p></p> <p>Confident that the Datalab can support his prototype development, Frank now turns to planning how best to structure storage and manage access.</p>"},{"location":"getting-started/quickstart/#5-frank-prepare-storage-layout-for-collaboration","title":"5) Frank: Prepare storage layout for collaboration","text":"<p>Frank begins by creating two additional buckets to organize data exchange and publication:</p> <ul> <li><code>ws-frank-stagein</code> \u2014 to be used by Bob to stage curated data subsets (e.g., filtered by geometry or time).  </li> <li><code>ws-frank-publish</code> \u2014 used by Frank himself to store and share finalized datasets with others.</li> </ul> <p></p> <p>Frank then adds Alice as a member of <code>ws-frank</code>, granting her access to all workspace resources.</p> <p></p> <p>With these permissions in place, Alice can now open Frank\u2019s workspace, view the necessary credentials, and collaborate with him in the shared Datalab environment.</p> <p>Next, Frank requests access to <code>ws-eric-shared</code> to use Eric\u2019s reference data.</p> <ul> <li>In the Workspace UI, he selects Request Access for the desired bucket.  </li> <li>Eric then approves the request.  </li> <li>Once approved, Frank can access <code>ws-eric-shared</code> seamlessly using his existing credentials.</li> </ul> <p></p> <p>Frank also notices that Bob has submitted a request to access <code>ws-frank-stagein</code>.  Frank reviews and approves the request.From that moment on, Bob can access <code>ws-frank-stagein</code> seamlessly using his existing credentials \u2014 unless Frank later decides to revoke the permission.</p> <p></p>"},{"location":"getting-started/quickstart/#6-bob-connects-stac-fastapi-pgstac-to-the-managed-database-instance-of-his-workspace","title":"6) Bob: Connects stac-fastapi-pgstac to the managed database instance of his workspace","text":"<p>Bob has created several managed PostgreSQL database instances via the Workspace UI.</p> <p></p> <p>Using the database credentials exposed on his dashboard page, he can directly connect applications such as stac-fastapi-pgstac to a database inside his workspace.</p> <p></p> <p>Each workspace exposes ready-to-use environment variables like <code>DATABASE_HOST</code>, <code>DATABASE_USER</code>, <code>DATABASE_PASSWORD</code> and similar. This allows applications to connect immediately without additional configuration. Also external access from outside of the Kubernetes cluster is possible.</p> <p>Note: The external Postgres endpoint is exposed through Envoy, which requires immediate TLS with SNI (direct TLS). The PostgreSQL server and libpq-based clients (e.g. psql, psycopg) fully support this. However, some non-libpq drivers such as asyncpg do not yet implement this negotiation correctly and may fail during connection setup.</p> <p>Bob uses stac-fastapi-pgstac to catalog curated datasets and expose them through a STAC API. Because he does not need the service running permanently, he starts it on demand inside the Datalab only when required.</p> <p>First, apply the following manifests to the Kubernetes cluster.</p> <pre><code>envsubst &lt;&lt;'EOF' | kubectl apply -f -\napiVersion: v1\nkind: Service\nmetadata:\n  name: stac-api\nspec:\n  selector:\n    app: stac-api\n  ports:\n    - name: http\n      port: 8080\n      targetPort: 8080\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stac-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: stac-api\n  template:\n    metadata:\n      labels:\n        app: stac-api\n    spec:\n      containers:\n        - name: api\n          image: ghcr.io/stac-utils/stac-fastapi-pgstac:6.2.0\n          ports:\n            - containerPort: 8080\n          env:\n            - name: PGHOST\n              value: ${DATABASE_HOST}\n            - name: PGPORT\n              value: \"${DATABASE_PORT}\"\n            - name: PGDATABASE\n              value: ${DATABASE_NAME}\n            - name: PGUSER\n              value: ${DATABASE_USER}\n            - name: PGPASSWORD\n              value: ${DATABASE_PASSWORD}\n          resources:\n            requests: { cpu: \"100m\", memory: \"256Mi\" }\n            limits:   { cpu: \"500m\", memory: \"1Gi\" }\nEOF\n</code></pre> <p>Now, initalize the database</p> <pre><code>kubectl run pgstac-migrate -it --rm \\\n  --image=ghcr.io/stac-utils/pgstac-pypgstac:v0.9.8 \\\n  --restart=Never \\\n  --env=\"PGHOST=${DATABASE_HOST}\" \\\n  --env=\"PGPORT=${DATABASE_PORT}\" \\\n  --env=\"PGDATABASE=${DATABASE_NAME}\" \\\n  --env=\"PGUSER=${DATABASE_USER}\" \\\n  --env=\"PGPASSWORD=${DATABASE_PASSWORD}\" \\\n  --command -- sh -lc 'pypgstac migrate'\n</code></pre> <p>Next, port-forward the stac-fastapi-pgstac service so that it appears in the Ports tab, allowing direct access to the service in the browser.</p> <pre><code>kubectl port-forward svc/stac-api 8080:8080\n</code></pre> <p>Now you can open <code>http://localhost:8080</code> and browse common endpoints like</p> <pre><code>/collections\n/search\n/collections/{id}/items\n</code></pre> <p>To cleanup, run</p> <pre><code>kubectl delete deploy stac-api\nkubectl delete svc stac-api\n</code></pre> <p>This keeps the service ephemeral and cost-efficient while still benefiting from a fully managed database that provides persistence and automated backups.</p>"},{"location":"getting-started/quickstart/#summary","title":"Summary","text":"<p>With the storage setup in place, everyone now understands where data should reside, and the collaboration begins.</p> <ul> <li>Frank downloads the necessary reference data from the <code>ws-eric-shared</code> bucket to start refining his prototype.  </li> <li>He then shares the selection criteria (e.g., geometry and time range) with Bob by preparing them online and placing the corresponding <code>.gpkg</code> file in his bucket.   Frank can simply send a presigned URL to Bob\u2019s team for direct access.  </li> <li>Bob subsequently copies the relevant ground-truth data into <code>ws-frank-stagein</code>.</li> </ul> <p>Note: Since Bob is also using Workspaces, he can easily perform this transfer using preinstalled tools such as <code>aws s3 sync</code> or <code>rclone sync</code>, ensuring efficient data movement.</p> <ul> <li> <p>Alice continues working collaboratively in the shared Datalab, starting her browser session directly from https://workspace-api.develop.eoepca.org/workspaces/ws-frank.  </p> </li> <li> <p>Once the prototype is complete and the first data products are generated, Frank synchronizes results from <code>ws-frank</code> to <code>ws-frank-publish</code>, ensuring a clear separation between working, staging, and published data.</p> </li> </ul>"},{"location":"getting-started/quickstart/#bonus-section-for-operators","title":"Bonus Section for Operators","text":"<p>The entire storage configuration is captured declaratively within the Kubernetes cluster. From there, it is continuously managed by reconciliation engines that enforce the desired state and ensure the setup remains consistent with the defined specification.</p> <p>The relevant portion of the storage manifest can be found directly after this quickstart, providing operators with a clear view of how the workspace and its associated buckets are provisioned and maintained.</p> <pre><code>apiVersion: pkg.internal/v1beta1\nkind: Storage\nmetadata:\n  name: ws-alice\n  namespace: workspace\nspec:\n  principal: alice\n  buckets:\n    - bucketName: ws-alice\n      discoverable: true\n    - bucketName: ws-alice-2\n      discoverable: true\n    - bucketName: ws-alice-3\n      discoverable: true\n  bucketAccessGrants:\n    - bucketName: ws-alice-3\n      grantee: eric\n      permission: ReadWrite\n      grantedAt: \"2025-10-14T08:09:10.817000+00:00\"\n  bucketAccessRequests:\n    - bucketName: ws-eric-shared\n      reason: requesting access\n      requestedAt: \"2025-10-19T13:25:59.655000+00:00\"\n---\napiVersion: pkg.internal/v1beta1\nkind: Storage\nmetadata:\n  name: ws-bob\n  namespace: workspace\nspec:\n  principal: bob\n  buckets:\n    - bucketName: ws-bob\n      discoverable: true\n  bucketAccessGrants: []\n  bucketAccessRequests:\n    - bucketName: ws-frank-stagein\n      reason: requesting access\n      requestedAt: \"2025-10-20T14:00:05.494000+00:00\"\n---\napiVersion: pkg.internal/v1beta1\nkind: Storage\nmetadata:\n  name: ws-eric\n  namespace: workspace\nspec:\n  principal: eric\n  buckets:\n    - bucketName: ws-eric\n      discoverable: true\n    - bucketName: ws-eric-shared\n      discoverable: true\n  bucketAccessGrants:\n    - bucketName: ws-eric-shared\n      grantee: alice\n      permission: ReadWrite\n      grantedAt: \"2025-10-19T13:28:45.636000+00:00\"\n    - bucketName: ws-eric-shared\n      grantee: frank\n      permission: ReadWrite\n      grantedAt: \"2025-10-20T14:32:22.153000+00:00\"\n  bucketAccessRequests:\n    - bucketName: ws-alice-3\n      reason: requesting access\n      requestedAt: \"2025-10-14T08:08:53.953000+00:00\"\n---\napiVersion: pkg.internal/v1beta1\nkind: Storage\nmetadata:\n  name: ws-frank\n  namespace: workspace\nspec:\n  principal: frank\n  buckets:\n    - bucketName: ws-frank\n      discoverable: true\n    - bucketName: ws-frank-stagein\n      discoverable: true\n    - bucketName: ws-frank-publish\n      discoverable: true\n  bucketAccessGrants:\n    - bucketName: ws-frank-stagein\n      grantee: bob\n      permission: ReadWrite\n      grantedAt: \"2025-10-20T14:01:49.246000+00:00\"\n  bucketAccessRequests:\n    - bucketName: ws-eric-shared\n      reason: requesting access\n      requestedAt: \"2025-10-20T14:32:10.939000+00:00\"\n</code></pre>"},{"location":"getting-started/user-view/","title":"User View","text":"In\u00a0[1]: Copied! <pre>import jwt\nimport requests\nimport datetime\nimport time\nimport os\nfrom dotenv import load_dotenv\nimport urllib3\nfrom pathlib import Path\n\nurllib3.disable_warnings()\n\nroot = Path.cwd()\nwhile not (root / \"eoepca-demo.env\").exists():\n    root = root.parent\n\nload_dotenv(root / \"eoepca-demo.env\")\n</pre> import jwt import requests import datetime import time import os from dotenv import load_dotenv import urllib3 from pathlib import Path  urllib3.disable_warnings()  root = Path.cwd() while not (root / \"eoepca-demo.env\").exists():     root = root.parent  load_dotenv(root / \"eoepca-demo.env\") Out[1]: <pre>True</pre> In\u00a0[2]: Copied! <pre>realm = os.getenv(\"REALM\")\nbase_domain = os.getenv(\"BASE_DOMAIN\")\nkeycloak_endpoint = os.getenv(\"KEYCLOAK_ENDPOINT\")\ndefault_scope = os.getenv(\"DEFAULT_SCOPE\")\nclient_id = os.getenv(\"CLIENT_ID\")\nclient_secret = os.getenv(\"CLIENT_SECRET\")\n\nissuer = f\"{keycloak_endpoint}/realms/{realm}\"\ntoken_endpoint = f\"{issuer}/protocol/openid-connect/token\"\ndevice_authorization_endpoint = f\"{issuer}/protocol/openid-connect/auth/device\"\nuserinfo_endpoint = f\"{issuer}/protocol/openid-connect/userinfo\"\n\nworkspace_api_endpoint = f'https://workspace-api.{base_domain}/workspaces'\n</pre> realm = os.getenv(\"REALM\") base_domain = os.getenv(\"BASE_DOMAIN\") keycloak_endpoint = os.getenv(\"KEYCLOAK_ENDPOINT\") default_scope = os.getenv(\"DEFAULT_SCOPE\") client_id = os.getenv(\"CLIENT_ID\") client_secret = os.getenv(\"CLIENT_SECRET\")  issuer = f\"{keycloak_endpoint}/realms/{realm}\" token_endpoint = f\"{issuer}/protocol/openid-connect/token\" device_authorization_endpoint = f\"{issuer}/protocol/openid-connect/auth/device\" userinfo_endpoint = f\"{issuer}/protocol/openid-connect/userinfo\"  workspace_api_endpoint = f'https://workspace-api.{base_domain}/workspaces' In\u00a0[3]: Copied! <pre># Helper function to get token via device flow\ndef token_via_device_flow(\n    client_id: str,\n    client_secret: str | None = None,\n    scope: str = \"openid profile email\",\n    poll_interval: int | None = None,\n    timeout: int = 600\n):\n    da_resp = requests.post(\n        device_authorization_endpoint,\n        headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n        data={\n            \"client_id\": client_id,\n            **({\"client_secret\": client_secret} if client_secret else {}),\n            \"scope\": scope,\n        },\n        timeout=30,\n    )\n    da_resp.raise_for_status()\n    da = da_resp.json()\n\n    device_code = da[\"device_code\"]\n    user_code = da[\"user_code\"]\n    verification_uri = da[\"verification_uri\"]\n    verification_uri_complete = da.get(\"verification_uri_complete\")\n    interval = poll_interval or da.get(\"interval\", 5)\n    expires_in = da.get(\"expires_in\", timeout)\n\n    print(f\"Open: {verification_uri}\")\n    print(f\"Enter code: {user_code}\")\n    if verification_uri_complete:\n        print(f\"(Or open directly): {verification_uri_complete}\")\n\n    start = time.time()\n    data = {\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",\n        \"device_code\": device_code,\n        \"client_id\": client_id,\n        **({\"client_secret\": client_secret} if client_secret else {}),\n    }\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n\n    while True:\n        if time.time() - start &gt; min(timeout, expires_in):\n            raise TimeoutError(\"Device authorization timed out.\")\n        resp = requests.post(token_endpoint, headers=headers, data=data, timeout=30)\n        if resp.ok:\n            tok = resp.json()\n            break\n        try:\n            err = resp.json().get(\"error\")\n        except Exception:\n            resp.raise_for_status()\n        if err == \"authorization_pending\":\n            time.sleep(interval)\n            continue\n        elif err == \"slow_down\":\n            interval += 5\n            time.sleep(interval)\n            continue\n        elif err in (\"access_denied\", \"expired_token\", \"invalid_grant\"):\n            raise RuntimeError(f\"Device flow failed: {err}\")\n        else:\n            resp.raise_for_status()\n\n    access_token = tok.get(\"access_token\")\n    decoded = jwt.decode(access_token, options={\"verify_signature\": False, \"verify_aud\": False})\n\n    print(\"\\n--- Decoded Access Token ---\")\n    for k, v in decoded.items():\n        print(f\"{k}: {v}\")\n\n    ra = decoded.get(\"resource_access\", {}) or {}\n    roles_wsapi = ra.get(client_id, {}).get(\"roles\", []) or []\n    is_operator = \"admin\" in roles_wsapi\n\n    workspaces = []\n    for client, meta in ra.items():\n        roles = (meta or {}).get(\"roles\", []) or []\n        if client.startswith(\"ws-\") and \"ws_access\" in roles:\n            workspaces.append(client)\n\n    print(\"\\n--- Access Summary ---\")\n    if is_operator:\n        print(\"user is OPERATOR\")\n    elif workspaces:\n        print(f\"user has WORKSPACE ACCESS: {', '.join(sorted(workspaces))}\")\n    else:\n        print(\"user is neither OPERATOR nor has WORKSPACE ACCESS\")\n\n    return access_token, is_operator, sorted(workspaces)\n</pre> # Helper function to get token via device flow def token_via_device_flow(     client_id: str,     client_secret: str | None = None,     scope: str = \"openid profile email\",     poll_interval: int | None = None,     timeout: int = 600 ):     da_resp = requests.post(         device_authorization_endpoint,         headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},         data={             \"client_id\": client_id,             **({\"client_secret\": client_secret} if client_secret else {}),             \"scope\": scope,         },         timeout=30,     )     da_resp.raise_for_status()     da = da_resp.json()      device_code = da[\"device_code\"]     user_code = da[\"user_code\"]     verification_uri = da[\"verification_uri\"]     verification_uri_complete = da.get(\"verification_uri_complete\")     interval = poll_interval or da.get(\"interval\", 5)     expires_in = da.get(\"expires_in\", timeout)      print(f\"Open: {verification_uri}\")     print(f\"Enter code: {user_code}\")     if verification_uri_complete:         print(f\"(Or open directly): {verification_uri_complete}\")      start = time.time()     data = {         \"grant_type\": \"urn:ietf:params:oauth:grant-type:device_code\",         \"device_code\": device_code,         \"client_id\": client_id,         **({\"client_secret\": client_secret} if client_secret else {}),     }     headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}      while True:         if time.time() - start &gt; min(timeout, expires_in):             raise TimeoutError(\"Device authorization timed out.\")         resp = requests.post(token_endpoint, headers=headers, data=data, timeout=30)         if resp.ok:             tok = resp.json()             break         try:             err = resp.json().get(\"error\")         except Exception:             resp.raise_for_status()         if err == \"authorization_pending\":             time.sleep(interval)             continue         elif err == \"slow_down\":             interval += 5             time.sleep(interval)             continue         elif err in (\"access_denied\", \"expired_token\", \"invalid_grant\"):             raise RuntimeError(f\"Device flow failed: {err}\")         else:             resp.raise_for_status()      access_token = tok.get(\"access_token\")     decoded = jwt.decode(access_token, options={\"verify_signature\": False, \"verify_aud\": False})      print(\"\\n--- Decoded Access Token ---\")     for k, v in decoded.items():         print(f\"{k}: {v}\")      ra = decoded.get(\"resource_access\", {}) or {}     roles_wsapi = ra.get(client_id, {}).get(\"roles\", []) or []     is_operator = \"admin\" in roles_wsapi      workspaces = []     for client, meta in ra.items():         roles = (meta or {}).get(\"roles\", []) or []         if client.startswith(\"ws-\") and \"ws_access\" in roles:             workspaces.append(client)      print(\"\\n--- Access Summary ---\")     if is_operator:         print(\"user is OPERATOR\")     elif workspaces:         print(f\"user has WORKSPACE ACCESS: {', '.join(sorted(workspaces))}\")     else:         print(\"user is neither OPERATOR nor has WORKSPACE ACCESS\")      return access_token, is_operator, sorted(workspaces)  In\u00a0[4]: Copied! <pre># Retrieve token via device flow\ntoken, is_operator, workspaces = token_via_device_flow(client_id=client_id, client_secret=client_secret)\n</pre> # Retrieve token via device flow token, is_operator, workspaces = token_via_device_flow(client_id=client_id, client_secret=client_secret) <pre>Open: https://iam-auth.develop.eoepca.org/realms/eoepca/device\nEnter code: XAZD-WXCB\n(Or open directly): https://iam-auth.develop.eoepca.org/realms/eoepca/device?user_code=XAZD-WXCB\n\n--- Decoded Access Token ---\nexp: 1768816188\niat: 1768815888\nauth_time: 1768815609\njti: eca3c600-5701-409a-94a1-28a08d7ac01c\niss: https://iam-auth.develop.eoepca.org/realms/eoepca\naud: ['ws-alice', 'account', 'ws-bob']\nsub: 5865cc63-abc6-4bf9-97d4-2d9ba7deec4c\ntyp: Bearer\nazp: workspace-api\nsid: 15edb74d-c815-48b4-be46-9e9e1398b7f8\nacr: 0\nallowed-origins: ['*']\nrealm_access: {'roles': ['offline_access', 'default-roles-eoepca', 'uma_authorization']}\nresource_access: {'ws-alice': {'roles': ['ws_admin', 'ws_access']}, 'account': {'roles': ['manage-account', 'view-consent', 'manage-account-links', 'manage-consent', 'view-profile']}, 'ws-bob': {'roles': ['ws_access']}}\nscope: openid profile email\nemail_verified: True\nname: Alice A\ngroups: ['/ws-alice', '/ws-alice-admin', '/ws-bob']\npreferred_username: alice\ngiven_name: Alice\nfamily_name: A\nemail: alice@eoepca.org\n\n--- Access Summary ---\nuser has WORKSPACE ACCESS: ws-alice, ws-bob\n</pre> In\u00a0[5]: Copied! <pre># Helper function to take screenshot using playwright\nfrom playwright.async_api import async_playwright\nfrom IPython.display import Image, display\n\nasync def snap_with_bearer_full(url: str, access_token: str, widen=True, fix_scroll=False):\n    async with async_playwright() as p:\n        browser = await p.chromium.launch(headless=True)\n        ctx = await browser.new_context(\n            extra_http_headers={\"Authorization\": f\"Bearer {access_token}\"},\n            viewport={\"width\": 1920, \"height\": 1080} if widen else None,\n            device_scale_factor=2  # crisp\n        )\n        page = await ctx.new_page()\n        await page.goto(url, wait_until=\"networkidle\")\n\n        if fix_scroll:\n            await page.add_style_tag(content=\"\"\"\n                html, body { height: auto !important; }\n                [style*=\"overflow\"], .scroll, .scrollable, .overflow-auto, .overflow-y-auto {\n                    overflow: visible !important; max-height: none !important; height: auto !important;\n                }\n            \"\"\")\n\n        img = await page.screenshot(full_page=True)  # entire page height\n        await browser.close()\n        display(Image(data=img))\n</pre> # Helper function to take screenshot using playwright from playwright.async_api import async_playwright from IPython.display import Image, display  async def snap_with_bearer_full(url: str, access_token: str, widen=True, fix_scroll=False):     async with async_playwright() as p:         browser = await p.chromium.launch(headless=True)         ctx = await browser.new_context(             extra_http_headers={\"Authorization\": f\"Bearer {access_token}\"},             viewport={\"width\": 1920, \"height\": 1080} if widen else None,             device_scale_factor=2  # crisp         )         page = await ctx.new_page()         await page.goto(url, wait_until=\"networkidle\")          if fix_scroll:             await page.add_style_tag(content=\"\"\"                 html, body { height: auto !important; }                 [style*=\"overflow\"], .scroll, .scrollable, .overflow-auto, .overflow-y-auto {                     overflow: visible !important; max-height: none !important; height: auto !important;                 }             \"\"\")          img = await page.screenshot(full_page=True)  # entire page height         await browser.close()         display(Image(data=img)) In\u00a0[6]: Copied! <pre>ws_name = \"ws-alice\" # exists on eopeca-demo environment and user should have access with admin permissions (like alice has for ws-alice)\nassert ws_name in workspaces, f\"User does not have access to workspace {ws_name}\"\n</pre> ws_name = \"ws-alice\" # exists on eopeca-demo environment and user should have access with admin permissions (like alice has for ws-alice) assert ws_name in workspaces, f\"User does not have access to workspace {ws_name}\" In\u00a0[7]: Copied! <pre># Check workspace creation status via API\nheaders = {\n    'Authorization': 'bearer ' + token\n}\nurl = f\"{workspace_api_endpoint}/{ws_name}\"\nprint(f\"HTTP GET {url}\")\nresponse = requests.get(url, headers=headers)\nassert response.status_code == 200\n</pre> # Check workspace creation status via API headers = {     'Authorization': 'bearer ' + token } url = f\"{workspace_api_endpoint}/{ws_name}\" print(f\"HTTP GET {url}\") response = requests.get(url, headers=headers) assert response.status_code == 200 <pre>HTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-alice\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[8]: Copied! <pre># Dashboard screen\nawait snap_with_bearer_full(url, token)\n</pre> # Dashboard screen await snap_with_bearer_full(url, token) In\u00a0[9]: Copied! <pre># Management screen\nawait snap_with_bearer_full(f\"{url}#/home/management\", token)\n</pre> # Management screen await snap_with_bearer_full(f\"{url}#/home/management\", token) In\u00a0[10]: Copied! <pre># Datalab screen (default session)\n# await snap_with_bearer_full(f\"{url}/sessions/default\", token)\n</pre> # Datalab screen (default session) # await snap_with_bearer_full(f\"{url}/sessions/default\", token) In\u00a0[11]: Copied! <pre>ws_name = \"ws-bob\" # exists on eopeca-demo environment and user should have access with user permissions (like alice has for ws-bob)\nassert ws_name in workspaces, f\"User does not have access to workspace {ws_name}\"\n</pre> ws_name = \"ws-bob\" # exists on eopeca-demo environment and user should have access with user permissions (like alice has for ws-bob) assert ws_name in workspaces, f\"User does not have access to workspace {ws_name}\" In\u00a0[12]: Copied! <pre># Check workspace  status via API\nheaders = {\n    'Authorization': 'bearer ' + token\n}\nurl = f\"{workspace_api_endpoint}/{ws_name}\"\nprint(f\"HTTP GET {url}\")\nresponse = requests.get(url, headers=headers)\nassert response.status_code == 200\n</pre> # Check workspace  status via API headers = {     'Authorization': 'bearer ' + token } url = f\"{workspace_api_endpoint}/{ws_name}\" print(f\"HTTP GET {url}\") response = requests.get(url, headers=headers) assert response.status_code == 200 <pre>HTTP GET https://workspace-api.develop.eoepca.org/workspaces/ws-bob\n</pre> In\u00a0[13]: Copied! <pre># Dashboard screen (no Management Tab!)\nawait snap_with_bearer_full(url, token)\n</pre> # Dashboard screen (no Management Tab!) await snap_with_bearer_full(url, token) In\u00a0[15]: Copied! <pre># Management screen (management route also not accessible via url)\nawait snap_with_bearer_full(f\"{url}#/home/management\", token)\n</pre> # Management screen (management route also not accessible via url) await snap_with_bearer_full(f\"{url}#/home/management\", token)"},{"location":"getting-started/user-view/#user-view","title":"User View\u00b6","text":"<p>This notebook demonstrates how a workspace user can access the workspaces they have been granted access to and explore related details such as required credentials, management pages, and available collaboration tools.</p> <p>Within the workspace, the user can:</p> <ul> <li>Invite additional members,</li> <li>Create new buckets for sharing data or for temporary scratch storage,</li> <li>Manage access to their own buckets (granting or revoking permissions),</li> <li>Request access to other shared buckets,</li> <li>And directly open the associated Datalab environment.</li> </ul> <p>Note: To perform these actions, the workspace user must be properly authenticated with the corresponding workspace access permissions \u2014 specifically, the <code>ws_admin</code> resp. <code>ws_access</code> role assigned to the dynamically created workspace client (which shares the same name as the workspace). Users <code>alice</code>, <code>bob</code>, <code>eric</code> have the required permissions on the EOEPCA demo system.</p> <p>These authorization mechanisms are governed by the EOEPCA IAM model, implemented through APISIX with the OpenID Connect plugin and Open Policy Agent (OPA) to enable fine-grained access control.</p>"},{"location":"usage/howtos/","title":"How-Tos","text":"<p>How-tos to communicate usage by example.</p>"},{"location":"usage/howtos/#workspace-building-block","title":"Workspace Building Block","text":""},{"location":"usage/howtos/#uc1-dedicated-workspace-for-users-and-projects","title":"UC1: Dedicated Workspace for Users and Projects","text":"<p>User Story: As a user, I want to use an instance of a building block or component (e.g., resource discovery, data access) which is dedicated to my own or my project workspace.</p> <p>Tech Standards: Kubernetes, GitOps</p> <p>Epic: E5310</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Create a Workspace: Use the Workspace Controller to create a new workspace dedicated to your project or individual use.</li> <li>Provision Services:  Utilize the Workspace Controller to provision the necessary services within your workspace, such as resource discovery, data access, or visualization tools.</li> <li>Access Services: Access the provisioned services within your workspace using the provided APIs or user interfaces.</li> </ol>"},{"location":"usage/howtos/#uc2-workspace-provisioning-and-management","title":"UC2: Workspace Provisioning and Management","text":"<p>User Story: As a platform operator, I want to leverage a SOTA solution to provision and manage workspaces for projects/groups.</p> <p>Tech Standards: Kubernetes, GitOps</p> <p>Epic: E5310</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Configure Workspace Templates: Define templates for different workspace types (e.g., individual user, project team) using GitOps principles.</li> <li>Provision Workspaces: Use the Workspace Controller to provision new workspaces based on the defined templates.</li> <li>Manage Workspaces: Monitor and manage workspace resources, including services, storage, and user access, through the Workspace Controller.</li> </ol>"},{"location":"usage/howtos/#storage-building-block","title":"Storage Building Block","text":""},{"location":"usage/howtos/#uc3-s3-object-storage-for-data-organization","title":"UC3: S3 Object Storage for Data Organization","text":"<p>User Story: As a user, I want an S3 object storage to organize and curate data.</p> <p>Tech Standards: S3</p> <p>Epic: E5350</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Create Storage Buckets: Use the Storage Controller to create new S3 buckets within your workspace.</li> <li>Upload Data: Upload your data files to the created buckets using the provided S3 API or HTTP access.</li> <li>Organize Data: Organize your data within the buckets using folders and file naming conventions.</li> </ol>"},{"location":"usage/howtos/#security-building-block","title":"Security Building Block","text":""},{"location":"usage/howtos/#uc4-iam-control-for-users","title":"UC4: IAM Control for Users","text":"<p>User Story: As a platform operator, I want to keep control on IAM for my users.</p> <p>Tech Standards: OAuth2</p> <p>Epic: E5340</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Configure IAM Roles: Define IAM roles with specific permissions for different user groups (e.g., data scientists, project managers).</li> <li>Assign Roles to Users: Assign the appropriate IAM roles to users based on their responsibilities and access needs.</li> <li>Monitor Access: Monitor user access and activity logs to ensure security and compliance.</li> </ol>"},{"location":"usage/howtos/#application-hub-building-block","title":"Application Hub Building Block","text":""},{"location":"usage/howtos/#uc5-delegated-workspace-service-instantiation","title":"UC5: Delegated Workspace Service Instantiation","text":"<p>User Story: As a platform operator, I want to delegate Workspace Service instantiation to the end-users without compromising security.</p> <p>Tech Standards: Kubernetes, Helm</p> <p>Epic: E5320</p> <p>Building Block: Workspace, Application Hub, MLOps</p> <p>How-To:</p> <ol> <li>Create Helm Charts: Package Workspace Services as Helm charts, including dependencies and configuration options.</li> <li>Publish Charts to Application Hub: Publish the Helm charts to the Application Hub, making them accessible to users.</li> <li>User Service Instantiation: Allow users to install and configure Workspace Services from the Application Hub using Helm.</li> </ol>"},{"location":"usage/howtos/#resource-management-building-block","title":"Resource Management Building Block","text":""},{"location":"usage/howtos/#uc6-runtime-resource-management","title":"UC6: Runtime Resource Management","text":"<p>User Story: As a platform operator, I want to easily turn on and off projects/groups runtime resources to save costs.</p> <p>Tech Standards: Kubernetes</p> <p>Epic: E5320</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Configure Resource Scaling: Define resource scaling policies for different workspace types or services.</li> <li>Automate Resource Management: Use Kubernetes features like autoscaling and resource quotas to automatically manage resource allocation based on usage patterns.</li> <li>Monitor Resource Usage: Monitor resource consumption and adjust scaling policies as needed to optimize costs.</li> </ol>"},{"location":"usage/howtos/#data-management-building-block","title":"Data Management Building Block","text":""},{"location":"usage/howtos/#uc7-data-integration-and-collaboration","title":"UC7: Data Integration and Collaboration","text":"<p>User Story: As a user, I want to integrate (copy as well as referencing) data in the project/group space for collaboration.</p> <p>Tech Standards: S3</p> <p>Epic: E5330</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Upload Data to Workspace Buckets: Upload data files to the workspace\u2019s S3 buckets.</li> <li>Share Data with Collaborators: Grant access to the workspace buckets to collaborators, allowing them to view, download, or modify data.</li> <li>Reference Data: Use data references (e.g., URLs, S3 paths) to link to data stored in the workspace buckets, enabling collaboration without duplicating data.</li> </ol>"},{"location":"usage/howtos/#uc8-data-discovery-and-exploration","title":"UC8: Data Discovery and Exploration","text":"<p>User Story: As a user, I want to have an exhaustive view on all available data in the project/group space.</p> <p>Epic: E5330</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Use Resource Discovery Service: Utilize the Workspace\u2019s resource discovery service to browse and search for available data within the workspace.</li> <li>Explore Data Metadata: Access metadata associated with data files, including file size, format, and creation date.</li> <li>Filter and Sort Data: Filter and sort data based on specific criteria to find relevant information.</li> </ol>"},{"location":"usage/howtos/#uc9-data-change-tracking","title":"UC9: Data Change Tracking","text":"<p>User Story: As a user, I want to be able to trace changes on data.</p> <p>Epic: E5330</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Enable Versioning: Configure versioning for the workspace\u2019s S3 buckets to track changes to data files.</li> <li>Access Data History: View previous versions of data files and track changes made over time.</li> <li>Restore Previous Versions: Restore previous versions of data files if needed.</li> </ol>"},{"location":"usage/howtos/#uc10-reproducible-data-exploration-and-processing","title":"UC10: Reproducible Data Exploration and Processing","text":"<p>User Story: As a user, I want to be able to explore / process / experiment with stable snapshots of data for reproducibility in a scalable way with common software libraries.</p> <p>Tech Standards: S3, fsspec</p> <p>Epic: E5330</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Create Data Snapshots: Create snapshots of data within the workspace\u2019s S3 buckets to ensure reproducibility.</li> <li>Use fsspec for Data Access: Utilize the fsspec library to access data snapshots from within your analysis environment.</li> <li>Scale Data Processing: Leverage the scalability of the underlying infrastructure to process large datasets efficiently.</li> </ol>"},{"location":"usage/howtos/#application-hub-building-block_1","title":"Application Hub Building Block","text":""},{"location":"usage/howtos/#uc11-familiar-data-libraries-and-tools","title":"UC11: Familiar Data Libraries and Tools","text":"<p>User Story: As a user, I want to use my familiar data library and tool stack for exploration and curation.</p> <p>Tech Standards: Kubernetes, Helm</p> <p>Epic: E5350</p> <p>Building Block: Workspace</p> <p>How-To:</p> <ol> <li>Install Data Libraries: Install your preferred data libraries and tools within your workspace using Helm charts from the Application Hub.</li> <li>Configure Libraries: Configure the installed libraries and tools to access data within the workspace.</li> <li>Use Familiar Tools: Utilize your familiar data libraries and tools for exploration, analysis, and curation.</li> </ol>"},{"location":"usage/howtos/#uc12-custom-applications-and-tools","title":"UC12: Custom Applications and Tools","text":"<p>User Story: As a user, I want to bring a custom set of existing applications and tools close to the data.</p> <p>Tech Standards: Kubernetes, Helm</p> <p>Epic: E5360</p> <p>Building Block: Workspace, Application Hub, MLOps</p> <p>How-To:</p> <ol> <li>Package Applications as Helm Charts: Package your custom applications and tools as Helm charts.</li> <li>Publish Charts to Application Hub: Publish the Helm charts to the Application Hub.</li> <li>Deploy Applications to Workspace: Deploy your custom applications and tools to your workspace using Helm.</li> </ol>"},{"location":"usage/tutorials/","title":"Tutorials","text":"<p>Tutorials as a learning aid.</p>"}]}